{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook Title: National Data Buoy Center (NDBC) Data Download**\n",
    "## This notebook performs the following task(s):\n",
    "> - #### Reads in historical, tabular data for a single buoy NDBC using pandas DataFrames. \n",
    "> - #### Creates a single pandas DataFrame with all the timeseries data for an individual buoy.\n",
    "> - #### Converts the pandas DataFrame into an xarray Dataset and exports the Dataset as a NetCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "#### **Links to documentation for packages:**\n",
    "> - #### [requests](https://requests.readthedocs.io/en/latest/) | [numpy](https://numpy.org/doc/1.21/) | [xarray](https://docs.xarray.dev/en/stable/) | [pandas](https://pandas.pydata.org/pandas-docs/version/1.3.5/) | [matplotlib](https://matplotlib.org/3.5.3/index.html) | \n",
    "> - #### Note #1: Package documentation versions linked above may not correspond to the exact package version used for this analysis.\n",
    "> - #### Note #2: Comments are also included in the actual code cells. Commented links above certain pieces of code are provided to help show where I found certain pieces of code that were either directly copied or adapted. It is possible that I missed some links, suggesting that there may be snippets of code I simply grabbed off the internet without attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "#Import packages\n",
    "import requests\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read historical NDBC data for a single buoy into pandas DataFrames and append each yearly DataFrame into a list\n",
    "\n",
    "#### **Notes**\n",
    "> - #### It is pretty neat that pandas allows us to specify a URL with tabular data, which can be read directly into a DataFrame.\n",
    "> - #### For information on the standard meteoroloigcal data used in this notebook from NDBC, please see the following [link](https://www.ndbc.noaa.gov/faq/measdes.shtml#stdmet).\n",
    "> - #### Please read in-line comments for additional details about this code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error: 404 Client Error: Not Found for url: https://www.ndbc.noaa.gov/view_text_file.php?filename=46053h1997.txt.gz&dir=data/historical/stdmet/\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------\n",
    "#Define buoy ID number\n",
    "#A few ID numbers around the Santa Barbara are are:\n",
    "#46218: Harvest buoy | 46054: West Santa Barbara buoy | 45053: East Santa Barbara buoy\n",
    "buoy_id        = 46053\n",
    "buoy_years     = np.arange(1994, 2023+1, 1)\n",
    "buoy_data_list = []\n",
    "#-------------------------------------------------\n",
    "#For every year we have, do the following:\n",
    "for buoy_year_index, buoy_year in enumerate(buoy_years):\n",
    "    \n",
    "    #Define URL to buoy data\n",
    "    #buoy_url = f'https://www.ndbc.noaa.gov/data/realtime2/{buoy_id}.txt'\n",
    "    buoy_url = f'https://www.ndbc.noaa.gov/view_text_file.php?filename={buoy_id}h{buoy_year}.txt.gz&dir=data/historical/stdmet/'\n",
    "    \n",
    "    #Station data before 1999 was identified to have two digits in the \"year\" column instead of four digits\n",
    "    #We add this \"if-else\" statement to check for this and make a correction to the date format character that we will use when reading the data into a pandas DataFrame\n",
    "    if buoy_year < 1999:\n",
    "        data_format_year = '%y'\n",
    "    else:\n",
    "        data_format_year = '%Y'\n",
    "    \n",
    "    #It appears that the historical buoy data from NDBC added a \"minute\" column to historical data beginning in 2005\n",
    "    #Historical data prior to 2005 is only available on an hourly basis, which makes reading the data into pandas slightly tricky.\n",
    "    #All good though. We just implement an \"if-else\" statement to take care of this assuming that all historical buoy data from NDBC is like this\n",
    "    #If the year is prior to 2005, read the data into pandas without specifying a \"minute\" column, else include a \"minute\" column\n",
    "    if buoy_year < 2005:\n",
    "        df_column_names = ['year', 'month', 'day', 'hour', 'wind_dir_deg', 'wind_spd_ms', 'wind_gst_ms', 'wave_height_m', 'dom_wave_period_sec', 'average_wave_period_sec', 'mean_wave_dir_deg', 'pressure_hpa', 'air_temp_c', 'water_temp_c', 'dewpoint_c']\n",
    "        df_date_format  = f'{data_format_year} %m %d %H'\n",
    "        df_date_indexes = [0, 1, 2, 3]\n",
    "        df_use_cols     = np.arange(0,14+1,1)\n",
    "    else: \n",
    "        df_column_names = ['year', 'month', 'day', 'hour', 'minute', 'wind_dir_deg', 'wind_spd_ms', 'wind_gst_ms', 'wave_height_m', 'dom_wave_period_sec', 'average_wave_period_sec', 'mean_wave_dir_deg', 'pressure_hpa', 'air_temp_c', 'water_temp_c', 'dewpoint_c']\n",
    "        df_date_format  = f'{data_format_year} %m %d %H %M'\n",
    "        df_date_indexes = [0, 1, 2, 3, 4]\n",
    "        df_use_cols     = np.arange(0,15+1,1)\n",
    "    \n",
    "    #It appears that in the NDBC historical data that the unit row was added to data files starting in 2007\n",
    "    #Because of this, we have to create and another \"if-else\" check to set how many rows we want to skip depending on what year we are in\n",
    "    if buoy_year < 2007:\n",
    "        skip_rows = 0\n",
    "    else:\n",
    "        skip_rows = 1\n",
    "    \n",
    "    #Use a \"try-except\" workflow to deal with an HTTPError that may occur if data is not available for a specific buoy during a specific year\n",
    "    #Thanks ChatGPT\n",
    "    try:\n",
    "        #Use the requests package to examine the current buoy URL\n",
    "        response = requests.get(buoy_url)\n",
    "        \n",
    "        #Check for HTTP errors\n",
    "        response.raise_for_status()  \n",
    "\n",
    "        #Read the data into a pandas DataFrame\n",
    "        #You will notice that there are a lot of custom options we specify when reading the buoy data using the \"read_csv\" function\n",
    "        #These were chosen based on ease of use for my personal python programming workflow\n",
    "        df = pd.read_csv(buoy_url, delim_whitespace=True, skiprows=skip_rows, header=0, na_values=['MM', 99.00, 999.0, 999, 9999.0], usecols=df_use_cols,\n",
    "                         names=df_column_names, parse_dates={'date':df_date_indexes}, date_format=df_date_format).set_index('date').sort_index(ascending=True)\n",
    "        \n",
    "        #Add DataFrame to list. We will concatentate all DataFrames into a single Dataframe after.\n",
    "        buoy_data_list.append(df)\n",
    "\n",
    "    #If there is an HTTPError, print it. \n",
    "    #This will not stop the code from continuing to run for other years, which is what we want.\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"HTTP Error: {errh}\")\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate pandas DataFrames for a single buoy into a single DataFrame, then convert to an xarray Dataset and export the final result as a NetCDF-4 file.\n",
    "\n",
    "#### **Notes**\n",
    "\n",
    "> - #### None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "#Concatenate all DataFrames in our list to form one large DataFrame\n",
    "concatenate_df = pd.concat(buoy_data_list)\n",
    "\n",
    "#Convert this DataFrame into an Xarray DataSet\n",
    "ds = xr.Dataset.from_dataframe(concatenate_df)\n",
    "\n",
    "#Export xarray Dataset to a NetCDF file\n",
    "ds.to_netcdf(path=f'ndbc_historical_{buoy_years[0]_to_{buoy_years[-1]}_{buoy_id}}', mode='w', format='NETCDF4')\n",
    "#-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (swex)",
   "language": "python",
   "name": "swex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
